# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k10GzB30wfXTlusHUMLupDK8xo8Ze6PZ
"""

from google.colab import files
uploaded = files.upload()

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Settings for plots
sns.set(style="whitegrid")
plt.rcParams.update({'figure.figsize':(8,5)})

# 2. Load Dataset
df = pd.read_csv('student.csv')

# 3. Initial Inspection and Cleaning
print("Initial Data Preview:\n", df.head())
print("\nShape:", df.shape)
print("\nInfo:")
print(df.info())
print("\nColumns before renaming:\n", df.columns)

# Rename for consistency
df.rename(columns={'Nacionality': 'Nationality', 'Age at enrollment': 'Age'}, inplace=True)
print("\nColumns after renaming:\n", df.columns)

# Handle duplicates
print("Duplicate rows:", df.duplicated().sum())
df.drop_duplicates(inplace=True)

# Missing values summary
print("\nMissing values per column:\n", df.isnull().sum())

# 4. Encode Target Variable for Analysis
le = LabelEncoder()
df['Target_enc'] = le.fit_transform(df['Target'])
print("\nTarget Encoding Mapping:")
for i, cls in enumerate(le.classes_):
    print(f"  {cls} -> {i}")

# 5. Descriptive stats
print("\nDescriptive statistics (numerical):")
print(df.describe().transpose())
print("\nDescriptive stats (all columns):")
print(df.describe(include='all').transpose())

# 6. Class Imbalance Detection
print("\nTarget class distribution:\n", df['Target'].value_counts())
plt.figure()
sns.countplot(x='Target', data=df)
plt.title('Target Class Distribution')
plt.show()

print("\nClass distribution (%):\n", df['Target'].value_counts(normalize=True) * 100)

plt.figure(figsize=(12,10))
corrmat = df.corr(numeric_only=True) # Added numeric_only=True to avoid warning
sns.heatmap(corrmat, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Correlation Heatmap')
plt.show()

# 7. Univariate Analysis

# Countplots of categorical variables (excluding target)
plt.figure(figsize=(6,4))
sns.countplot(x='Target', data=df)
plt.title('Distribution of Target Classes')
plt.xlabel('Target')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Gender', data=df)
plt.title('Distribution of Gender')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Displaced', data=df)
plt.title('Distribution of Displaced')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Educational special needs', data=df)
plt.title('Distribution of Educational special needs')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Marital status', data=df)
plt.title('Distribution of Marital status')
plt.show()

# Categorical variables distributions (excluding Target)
cat_cols = df.select_dtypes(include='object').columns.drop('Target')
for col in cat_cols:
    plt.figure()
    sns.countplot(x=col, data=df)
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45)
    plt.show()

# Numeric variables distributions
num_cols = df.select_dtypes(include=np.number).columns.drop('Target_enc')
for col in num_cols:
    plt.figure()
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f'Distribution of {col}')
    plt.show()

# 8. Outlier Detection & Handling using IQR method
Q1 = df['Age'].quantile(0.25)
Q3 = df['Age'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5*IQR
upper_bound = Q3 + 1.5*IQR
outliers = df[(df['Age'] < lower_bound) | (df['Age'] > upper_bound)]
print(f"Outliers in Age before replacement: {len(outliers)}")

# Replace outliers in Age with median
median_age = df['Age'].median()
df.loc[df['Age'] < lower_bound, 'Age'] = median_age
df.loc[df['Age'] > upper_bound, 'Age'] = median_age
print("Outliers replaced with median in Age.")

# Visualize boxplot after handling
plt.figure()
sns.boxplot(x=df['Age'])
plt.title('Boxplot of Age (Outliers Handled)')
plt.show()

# 9. Bivariate Analysis
# Numeric vs Target_enc using boxplot and violinplot
plt.figure(figsize=(7,4))
sns.boxplot(x='Target', y='Age', data=df)
plt.title('Age Distribution by Dropout Outcome')
plt.show()

plt.figure()
sns.boxplot(x='Target', y='Age', data=df)
plt.title('Age Distribution by Target')
plt.show()

plt.figure()
sns.violinplot(x='Target', y='Admission grade', data=df)
plt.title('Admission Grade by Target')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Debtor', data=df)
plt.title('Distribution of Debtor')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Tuition fees up to date', data=df)
plt.title('Distribution of Tuition fees up to date')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='Scholarship holder', data=df)
plt.title('Distribution of Scholarship holder')
plt.show()

plt.figure(figsize=(5,3))
sns.countplot(x='International', data=df)
plt.title('Distribution of International')
plt.show()

# Categorical vs Target using stacked bar plots
for col in ['Gender', 'Displaced', 'Educational special needs', 'Debtor',
            'Tuition fees up to date', 'Scholarship holder', 'International']:
    plt.figure()
    ct = pd.crosstab(df[col], df['Target'])
    ct.plot(kind='bar', stacked=True, colormap='viridis', figsize=(6,4))
    plt.title(f'Target distribution across {col}')
    plt.ylabel('Count')
    plt.show()

# Cross-tab heatmap example for Marital status
ct_marital = pd.crosstab(df['Marital status'], df['Target'])
plt.figure(figsize=(8,4))
sns.heatmap(ct_marital, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Marital Status vs Target')
plt.show()

# 10. Multivariate Analysis
# Corrected column names for pairplot
features_p = ['Age', 'Admission grade', 'Previous qualification (grade)', 'Target']
sns.pairplot(df[features_p], hue='Target', diag_kind='kde')
plt.suptitle('Pairplot of Key Numeric Features by Target', y=1.02)
plt.show()

# Correlation matrix and heatmap
plt.figure(figsize=(12,10))
corrmat = df.corr(numeric_only=True) # Added numeric_only=True since categorical values cannot be compared with numeric columns
sns.heatmap(corrmat, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title('Correlation Heatmap')
plt.show()

# 11. Feature Selection based on correlation with target
corr_with_target = corrmat['Target_enc'].drop(['Target_enc'])
print("\nCorrelation with Target_enc:\n", corr_with_target)

weak_features = corr_with_target[abs(corr_with_target) < 0.1].index.tolist()
print("\nWeakly correlated features (|corr|<0.1) dropped:", weak_features)
df.drop(columns=weak_features, inplace=True) # Dropping weak features after analyzing the pairplot

print("\nShape after dropping weak features:", df.shape)

# 12. Feature Engineering

# Binning Age into categories
age_bins = [15, 18, 22, 25, 30, 40, 60]
age_labels = ['<18', '18-22', '23-25', '26-30', '31-40', '41+']
df['Age_bin'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels)

plt.figure()
sns.countplot(x='Age_bin', hue='Target', data=df)
plt.title('Target by Age Group')
plt.show()

# Identify categorical variables
cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
cat_cols = [col for col in cat_cols if col not in ['Target']]  # Exclude target if still present

# One-hot encode
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)
print("\nShape after one-hot encoding:", df.shape)

#Can help capture non-linear relationships
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['Admission grade']])
poly_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['Admission grade']))
df = pd.concat([df, poly_df.drop(columns='Admission grade')], axis=1)

# Drop original 'Admission grade' column
df.drop(columns=['Admission grade'], inplace=True)

# Example transformation: logarithm of 'Previous qualification grade' (after verifying positive values)
if (df['Previous qualification (grade)'] > 0).all():
    df['Prev_qual_log'] = np.log(df['Previous qualification (grade)'])
    plt.figure()
    sns.histplot(df['Prev_qual_log'], kde=True)
    plt.title('Log-transformed Previous Qualification Grade')
    plt.show()

# Example of feature extraction or interaction feature (if meaningful)
# e.g., Total failures vs Admission grade could be combined or transformed as needed

# 13. Summary statistics grouped by Target
print("\nSummary statistics grouped by Target:")
print(df.groupby('Target').describe().transpose())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ML Preprocessing, Modeling
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Samplers from imblearn
from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN, SMOTETomek
from imblearn.pipeline import Pipeline as ImbPipeline

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, StackingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
import xgboost as xgb

X = df.drop(columns=['Target', 'Target_enc'])
y = df['Target_enc']

# Encode categorical features for ML
cat_features = X.select_dtypes(include='object').columns
for col in cat_features:
    X[col] = LabelEncoder().fit_transform(X[col].astype(str))

# Train/test split (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

# Scale numeric features (useful for some models)
num_features = X.select_dtypes(include=np.number).columns
scaler = StandardScaler()
X_train[num_features] = scaler.fit_transform(X_train[num_features])
X_test[num_features] = scaler.transform(X_test[num_features])

# ----------------------------------------------
# 2. Define models and their hyperparameter grids
# ----------------------------------------------
models_params_resamplers = [
    # Models with class_weight support; resampling is optional
    {
        'name': 'Logistic Regression',
        'model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),
        'param_grid': {'C': [0.01, 0.1, 1, 10]},
        'resampler': None   # No resampling, rely on class_weight
    },
    {
        'name': 'Decision Tree',
        'model': DecisionTreeClassifier(class_weight='balanced', random_state=42),
        'param_grid': {'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5, 10]},
        'resampler': None
    },
    {
        'name': 'Random Forest',
        'model': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42),
        'param_grid': {'max_depth': [None, 10, 20], 'min_samples_split': [2, 5]},
        'resampler': None
    },
    {
        'name': 'Support Vector Machine',
        'model': SVC(class_weight='balanced', probability=True, random_state=42),
        'param_grid': {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']},
        'resampler': None
    },
    # Models without class_weight: use resampling (e.g., SMOTE)
    {
        'name': 'K-Nearest Neighbors',
        'model': KNeighborsClassifier(),
        'param_grid': {'n_neighbors': [3, 5, 7]},
        'resampler': SMOTE(random_state=42)
    },
    {
        'name': 'Naive Bayes',
        'model': GaussianNB(),
        'param_grid': {},
        'resampler': SMOTE(random_state=42)
    },
    {
        'name': 'AdaBoost',
        'model': AdaBoostClassifier(random_state=42),
        'param_grid': {'n_estimators': [50, 100], 'learning_rate': [0.5, 1.0]},
        'resampler': SMOTE(random_state=42)
    },
    {
        'name': 'Bagging',
        'model': BaggingClassifier(random_state=42),
        'param_grid': {'n_estimators': [10, 50]},
        'resampler': SMOTE(random_state=42)
    },
    {
        'name': 'XGBoost',
        'model': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
        'param_grid': {'max_depth': [3, 6], 'learning_rate': [0.01, 0.1], 'n_estimators': [50, 100]},
        'resampler': None
    },
]

# -------------------------------
# 3. Unified training and evaluation
# -------------------------------
def train_evaluate(name, model, param_grid, resampler=None):
    print(f"\n{name} Training and Evaluation")
    # Build Pipeline
    if resampler is not None:
        pipeline = ImbPipeline([
            ("resampler", resampler),
            ("clf", model)
        ])
        param_grid_pipe = {"clf__" + k: v for k, v in param_grid.items()}
        estimator = pipeline
        param_grid_to_use = param_grid_pipe
    else:
        estimator = model
        param_grid_to_use = param_grid

    # Grid search with stratified 5-fold CV (resampling inside CV folds if pipeline)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    gs = GridSearchCV(
        estimator, param_grid=param_grid_to_use, scoring='f1_weighted', cv=cv, n_jobs=-1
    )
    gs.fit(X_train, y_train)
    best_model = gs.best_estimator_
    print(f"Best Params: {gs.best_params_}")

    # Predict on untouched test set (NO resampling)
    y_pred = best_model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    print(f"Test Accuracy: {acc:.4f}, Weighted F1-score: {f1:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)

    # Plot Confusion Matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix for {name}')
    plt.show()

    print("-" * 60)

# ---------------------------------------
# 4. Run all models (per their resamplers)
# ---------------------------------------
for entry in models_params_resamplers:
    train_evaluate(
        entry['name'],
        entry['model'],
        entry['param_grid'],
        resampler=entry.get('resampler')
    )

# -------------------------------
# 5. Stacking Classifier Example
# -------------------------------
from sklearn.base import clone

base_estimators = [
    ('lr', clone(models_params_resamplers[0]['model'])),
    ('dt', clone(models_params_resamplers[1]['model'])),
    ('knn', clone(models_params_resamplers[4]['model'])),
    ('rf', clone(models_params_resamplers[2]['model'])),
]
stacking_clf = StackingClassifier(
    estimators=base_estimators,
    final_estimator=LogisticRegression(),
    cv=5,
    n_jobs=-1,
    passthrough=True
)

print('\nStacking Classifier (fit on original, not resampled):')
stacking_clf.fit(X_train, y_train)
y_pred_stack = stacking_clf.predict(X_test)
print(classification_report(y_test, y_pred_stack, target_names=le.classes_))

# Confusion Matrix for stacking
cm_stack = confusion_matrix(y_test, y_pred_stack)
print("Confusion Matrix for Stacking Classifier:")
print(cm_stack)

disp_stack = ConfusionMatrixDisplay(confusion_matrix=cm_stack, display_labels=le.classes_)
disp_stack.plot(cmap=plt.cm.Greens)
plt.title('Confusion Matrix for Stacking Classifier')
plt.show()